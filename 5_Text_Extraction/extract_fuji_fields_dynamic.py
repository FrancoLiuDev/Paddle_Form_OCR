#!/usr/bin/env python3
"""
å‹•æ…‹æå– fuji.png ä¸­çš„å°è¡¨æ©Ÿè³‡è¨Šæ¬„ä½
ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…è‡ªå‹•åˆ¤æ–·ï¼Œä¸éœ€è¦é å…ˆå®šç¾©å€™é¸åå–®
"""
import json
import re
from difflib import SequenceMatcher

def fuzzy_match(text: str, target: str, threshold: float = 0.6) -> float:
    """
    è¨ˆç®—å…©å€‹å­—ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆæ”¹é€²ç‰ˆï¼‰
    
    æ”¯æ´éƒ¨åˆ†åŒ¹é…ï¼šå¦‚æžœ text åŒ…å« target çš„ä¸»è¦éƒ¨åˆ†ï¼Œä¹Ÿç®—åŒ¹é…
    
    Args:
        text: å¾…æ¯”å°å­—ä¸²
        target: ç›®æ¨™å­—ä¸²
        threshold: ç›¸ä¼¼åº¦é–€æª»
    
    Returns:
        ç›¸ä¼¼åº¦åˆ†æ•¸ (0-1)
    """
    # æ–¹æ³• 1: å®Œæ•´å­—ä¸²ç›¸ä¼¼åº¦
    full_similarity = SequenceMatcher(None, text, target).ratio()
    
    # æ–¹æ³• 2: åŽ»æŽ‰å¸¸è¦‹å‰ç¶´å¾Œæ¯”å°
    # ä¾‹å¦‚ï¼š"ç¸½å°å¼µæ•¸" â†’ "å°å¼µæ•¸" or "å°å¼ æ•°"
    prefixes = ['ç¸½', 'å…¨', 'æ€»', 'å…¨éƒ¨', 'æ‰€æœ‰']
    
    max_similarity = full_similarity
    
    for prefix in prefixes:
        if target.startswith(prefix):
            # åŽ»æŽ‰å‰ç¶´å¾Œçš„ç›®æ¨™
            target_without_prefix = target[len(prefix):]
            # è¨ˆç®—ç›¸ä¼¼åº¦
            sim = SequenceMatcher(None, text, target_without_prefix).ratio()
            max_similarity = max(max_similarity, sim)
    
    # æ–¹æ³• 3: ç¹ç°¡è½‰æ›å¾Œæ¯”å°
    # å¼µâ†’å¼ , æ•¸â†’æ•°, æ©Ÿâ†’æœº
    conversions = [
        ('å¼µ', 'å¼ '), ('æ•¸', 'æ•°'), ('æ©Ÿ', 'æœº'),
        ('ç¨±', 'ç§°'), ('è™Ÿ', 'å·'), ('èˆ‡', 'ä¸Ž')
    ]
    
    for trad, simp in conversions:
        if trad in target:
            target_simp = target.replace(trad, simp)
            sim = SequenceMatcher(None, text, target_simp).ratio()
            max_similarity = max(max_similarity, sim)
            
            # åŒæ™‚å˜—è©¦åŽ»æŽ‰å‰ç¶´
            for prefix in prefixes:
                if target_simp.startswith(prefix):
                    target_clean = target_simp[len(prefix):]
                    sim = SequenceMatcher(None, text, target_clean).ratio()
                    max_similarity = max(max_similarity, sim)
    
    # æ–¹æ³• 4: åŒ…å«æª¢æŸ¥ï¼ˆé—œéµè©žåŒ¹é…ï¼‰
    # å¦‚æžœ text åŒ…å«åœ¨ target ä¸­ï¼Œæˆ– target çš„ä¸€éƒ¨åˆ†åœ¨ text ä¸­
    if text in target or target in text:
        # æ ¹æ“šé•·åº¦æ¯”ä¾‹è¨ˆç®—ç›¸ä¼¼åº¦
        max_similarity = max(max_similarity, 
                            min(len(text), len(target)) / max(len(text), len(target)))
    
    return max_similarity

def find_field_by_fuzzy_match(text_blocks, field_name, threshold=0.5):
    """
    ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…å‹•æ…‹å°‹æ‰¾æ¬„ä½
    
    Args:
        text_blocks: OCRæ–‡å­—å€å¡Šåˆ—è¡¨
        field_name: è¦å°‹æ‰¾çš„æ¬„ä½åç¨±
        threshold: ç›¸ä¼¼åº¦é–€æª»
    
    Returns:
        åŒ¹é…åˆ°çš„å€å¡Šç´¢å¼•å’Œç›¸ä¼¼åº¦
    """
    best_match = None
    best_similarity = 0
    
    for i, block in enumerate(text_blocks):
        text = block['text'].strip()
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        similarity = fuzzy_match(text, field_name, threshold)
        
        # å¦‚æžœç›¸ä¼¼åº¦è¶…éŽé–€æª»ä¸”æ›´é«˜
        if similarity >= threshold and similarity > best_similarity:
            best_similarity = similarity
            best_match = {
                'index': i,
                'text': text,
                'similarity': similarity,
                'confidence': block['confidence']
            }
    
    return best_match

def find_value_by_position(text_blocks, field_index, search_range=5):
    """
    æ ¹æ“šæ¬„ä½ä½ç½®æ‰¾å³é‚Šçš„å€¼
    
    Args:
        text_blocks: OCRæ–‡å­—å€å¡Šåˆ—è¡¨
        field_index: æ¬„ä½çš„ç´¢å¼•
        search_range: æœå°‹ç¯„åœ
    
    Returns:
        æ‰¾åˆ°çš„å€¼è³‡è¨Š
    """
    if field_index >= len(text_blocks):
        return None
    
    field = text_blocks[field_index]
    bbox = field['bbox']
    field_x = bbox[1][0]  # æ¬„ä½å³é‚Šçš„ x åº§æ¨™
    field_y = (bbox[0][1] + bbox[2][1]) / 2  # æ¬„ä½ y ä¸­å¿ƒ
    
    candidates = []
    
    for i in range(field_index + 1, min(field_index + search_range, len(text_blocks))):
        next_block = text_blocks[i]
        next_bbox = next_block['bbox']
        next_x = next_bbox[0][0]
        next_y = (next_bbox[0][1] + next_bbox[2][1]) / 2
        
        # æª¢æŸ¥æ˜¯å¦åœ¨åŒä¸€è¡Œä¸”åœ¨å³é‚Š
        y_diff = abs(next_y - field_y)
        if next_x > field_x and y_diff < 20:
            candidates.append({
                'value': next_block['text'].strip(),
                'confidence': next_block['confidence'],
                'distance': next_x - field_x,
                'y_diff': y_diff
            })
    
    # é¸æ“‡æœ€æŽ¥è¿‘çš„å€™é¸å€¼
    if candidates:
        candidates.sort(key=lambda x: (x['y_diff'], x['distance']))
        return candidates[0]
    
    return None

def extract_field_dynamic(text_blocks, field_name, threshold=0.5):
    """
    å‹•æ…‹æå–æ¬„ä½å€¼
    
    Args:
        text_blocks: OCRæ–‡å­—å€å¡Šåˆ—è¡¨
        field_name: æ¬„ä½åç¨±
        threshold: ç›¸ä¼¼åº¦é–€æª»
    
    Returns:
        æå–çµæžœ
    """
    # 1. ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…æ‰¾æ¬„ä½
    field_match = find_field_by_fuzzy_match(text_blocks, field_name, threshold)
    
    if not field_match:
        return None
    
    # 2. æ ¹æ“šä½ç½®æ‰¾å°æ‡‰çš„å€¼
    value_info = find_value_by_position(text_blocks, field_match['index'])
    
    if not value_info:
        return None
    
    return {
        'field_text': field_match['text'],
        'field_similarity': field_match['similarity'],
        'field_confidence': field_match['confidence'],
        'value': value_info['value'],
        'value_confidence': value_info['confidence']
    }

def semantic_field_detection(text_blocks):
    """
    ä½¿ç”¨èªžç¾©åˆ†æžè‡ªå‹•åµæ¸¬å¯èƒ½çš„æ¬„ä½
    
    Returns:
        æª¢æ¸¬åˆ°çš„æ¬„ä½åˆ—è¡¨
    """
    detected_fields = []
    
    # å¸¸è¦‹æ¬„ä½çš„èªžç¾©ç‰¹å¾µ
    semantic_patterns = {
        'å°è¡¨æ©Ÿç›¸é—œ': ['å°è¡¨', 'æ‰“å°', 'åˆ—å°', 'æ©Ÿåž‹', 'åž‹è™Ÿ'],
        'æ•¸é‡ç›¸é—œ': ['å¼µæ•¸', 'æ¬¡æ•¸', 'é æ•¸', 'å°', 'æ•¸'],
        'è­˜åˆ¥ç¢¼ç›¸é—œ': ['åºè™Ÿ', 'ç·¨è™Ÿ', 'SN', 'Serial', 'åºåˆ—'],
        'æ—¥æœŸæ™‚é–“': ['æ—¥æœŸ', 'æ™‚é–“', 'å¹´', 'æœˆ', 'æ—¥'],
        'ç¶²è·¯ç›¸é—œ': ['IP', 'ä½å€', 'DHCP', 'ç¶²è·¯']
    }
    
    for i, block in enumerate(text_blocks):
        text = block['text'].strip()
        
        # è·³éŽå¤ªçŸ­çš„æ–‡å­—
        if len(text) < 2:
            continue
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ¬„ä½ç‰¹å¾µ
        for category, keywords in semantic_patterns.items():
            for keyword in keywords:
                if keyword.lower() in text.lower():
                    detected_fields.append({
                        'index': i,
                        'text': text,
                        'category': category,
                        'keyword': keyword,
                        'confidence': block['confidence']
                    })
                    break
    
    return detected_fields

def main():
    # è®€å– OCR çµæžœ
    json_file = '../4_OCR_Recognition/result/result_fuji_test.json'
    
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    text_blocks = data.get('text_blocks', [])
    
    print('\n' + '='*70)
    print('ðŸ¤– å‹•æ…‹æ¬„ä½æå– - ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…å’Œèªžç¾©åˆ†æž')
    print('='*70 + '\n')
    
    # è¦æå–çš„æ¬„ä½ï¼ˆåªéœ€è¦ç›®æ¨™åç¨±ï¼Œä¸éœ€è¦å€™é¸åˆ—è¡¨ï¼‰
    target_fields = {
        'å°è¡¨æ©Ÿåç¨±': 0.5,  # æ¬„ä½åç¨±: ç›¸ä¼¼åº¦é–€æª»
        'ç¸½å°å¼µæ•¸': 0.4,
        'å½©è‰²å°å¼µæ•¸': 0.5,
        'é»‘ç™½å°å¼µæ•¸': 0.5,
        'åºè™Ÿ': 0.4
    }
    
    results = {}
    
    print('ðŸ“‹ æ–¹æ³• 1: æ¨¡ç³ŠåŒ¹é…æå–')
    print('-'*70)
    
    # å‹•æ…‹æå–æ¯å€‹æ¬„ä½
    for field_name, threshold in target_fields.items():
        result = extract_field_dynamic(text_blocks, field_name, threshold)
        
        if result:
            results[field_name] = result
            print(f'âœ… ã€{field_name}ã€‘')
            print(f'   æ¬„ä½è­˜åˆ¥: "{result["field_text"]}" (ç›¸ä¼¼åº¦: {result["field_similarity"]:.1%})')
            print(f'   å€¼: "{result["value"]}" (ä¿¡å¿ƒåº¦: {result["value_confidence"]*100:.1f}%)')
            print()
        else:
            print(f'âŒ ã€{field_name}ã€‘æœªæ‰¾åˆ° (é–€æª»: {threshold})')
            print()
    
    # èªžç¾©åˆ†æžï¼šè‡ªå‹•åµæ¸¬æ‰€æœ‰å¯èƒ½çš„æ¬„ä½
    print('\n' + '='*70)
    print('ðŸ“‹ æ–¹æ³• 2: èªžç¾©åˆ†æž - è‡ªå‹•åµæ¸¬æ¬„ä½')
    print('-'*70)
    
    detected = semantic_field_detection(text_blocks)
    
    # æŒ‰é¡žåˆ¥åˆ†çµ„
    by_category = {}
    for field in detected:
        category = field['category']
        if category not in by_category:
            by_category[category] = []
        by_category[category].append(field)
    
    for category, fields in by_category.items():
        print(f'\nðŸ·ï¸  {category}:')
        for field in fields[:3]:  # åªé¡¯ç¤ºå‰ 3 å€‹
            print(f'   â€¢ "{field["text"]}" (åŒ¹é…é—œéµå­—: {field["keyword"]})')
    
    print('\n' + '='*70)
    print(f'âœ¨ æ¨¡ç³ŠåŒ¹é…æˆåŠŸæå– {len(results)}/{len(target_fields)} å€‹æ¬„ä½')
    print(f'âœ¨ èªžç¾©åˆ†æžåµæ¸¬åˆ° {len(detected)} å€‹å¯èƒ½çš„æ¬„ä½')
    print('='*70 + '\n')
    
    # å„²å­˜çµæžœ
    output = {
        'extraction_method': 'dynamic_fuzzy_matching',
        'extracted_fields': {
            field: {
                'value': info['value'],
                'value_confidence': f'{info["value_confidence"]*100:.1f}%',
                'field_text': info['field_text'],
                'field_similarity': f'{info["field_similarity"]:.1%}'
            }
            for field, info in results.items()
        },
        'detected_fields': [
            {
                'text': f['text'],
                'category': f['category']
            }
            for f in detected
        ],
        'total_extracted': len(results),
        'total_detected': len(detected)
    }
    
    output_file = 'result/fuji_printer_info_dynamic.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f'ðŸ’¾ çµæžœå·²å„²å­˜è‡³: {output_file}')

if __name__ == '__main__':
    main()
